{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "violent-binary",
   "metadata": {},
   "source": [
    "# Algorithm usage tutorial\n",
    "\n",
    "During the development of a reinforcement learning application, we often train the agent using different reinforcement learning algorithm and see its performance difference.\n",
    "In this notebook, you will learn how to train an agent in the same environment using different algorithms.  \n",
    "3 steps.\n",
    "\n",
    "(0. Preparation of this notebook)\n",
    "1. Setting up the training environment \n",
    "2. Setup the DDPG algorithm and train\n",
    "3. Change the training algorithm to SAC and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-christmas",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's start by first installing nnabla-rl and importing required packages for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nnabla-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import nnabla as nn\n",
    "from nnabla import functions as NF\n",
    "from nnabla import parametric_functions as NPF\n",
    "from nnabla import solvers as NS\n",
    "\n",
    "import nnabla_rl\n",
    "import nnabla_rl.algorithms as A\n",
    "import nnabla_rl.writers as W\n",
    "import nnabla_rl.functions as RF\n",
    "from nnabla_rl.builders import SolverBuilder\n",
    "from nnabla_rl.environments.environment_info import EnvironmentInfo\n",
    "from nnabla_rl.models.q_function import QFunction\n",
    "from nnabla_rl.environments.wrappers import NumpyFloat32Env, ScreenRenderEnv\n",
    "from nnabla_rl.utils.evaluator import EpisodicEvaluator\n",
    "from nnabla_rl.utils.reproductions import set_global_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sony/nnabla-rl.git\n",
    "!bash nnabla-rl/interactive-demos/package_install.sh\n",
    "%run nnabla-rl/interactive-demos/colab_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.clear_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impaired-climb",
   "metadata": {},
   "source": [
    "## Setting up the training environment\n",
    "\n",
    "Set up the \"Pendulum\" environment provided by the OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = NumpyFloat32Env(env)\n",
    "    env = ScreenRenderEnv(env)  # for rendering environment\n",
    "    env.seed(0) # optional\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Pendulum-v0\"\n",
    "env = build_env(env_name)\n",
    "set_global_seed(0) # optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-packet",
   "metadata": {},
   "source": [
    "## Preparation of Hook (optional)\n",
    "\n",
    "This hook may slow down the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_hook = RenderHook(env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-scottish",
   "metadata": {},
   "source": [
    "## Setup the DDPG algorithm and train\n",
    "\n",
    "We are almost ready to start the training. Let's first try the DDPG algorithm to train the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = A.DDPGConfig(gpu_id=0, start_timesteps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = A.DDPG(\n",
    "    env_or_env_info=env,\n",
    "    config=config\n",
    ")\n",
    "ddpg.set_hooks([render_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg.train(env, total_iterations=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-wilson",
   "metadata": {},
   "source": [
    "Wait for a while and see that the pendulum swang up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-compensation",
   "metadata": {},
   "source": [
    "## Change the training algorithm to SAC and train\n",
    "\n",
    "Next, let's try training the agent with another reinforcement learning algorithm SAC.  \n",
    "You will find that changing the algorithm is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "nn.clear_parameters()\n",
    "render_hook.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = A.SACConfig(gpu_id=gpu_id, start_timesteps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac = A.SAC(\n",
    "    env_or_env_info=env,\n",
    "    config=config\n",
    ")\n",
    "sac.set_hooks([render_hook])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac.train(env, total_iterations=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-account",
   "metadata": {},
   "source": [
    "Changing the training algorithm is easy, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-terry",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "To train an agent using different algorithms, you'll need to check the action type required by the environment.  \n",
    "Required action type must be supported by the algorithm that you want to use.  \n",
    "In this example, we used the \"Pendulum\" environment which works with continuous action outputs and both DDPG and SAC supports continuous action environment.\n",
    "\n",
    "See the [Algorithm catalog](https://github.com/sony/nnabla-rl/blob/master/nnabla_rl/algorithms/README.md) for the action type supported by the algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "accelerator": "GPU",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
