{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom environment tutorial\n",
    "\n",
    "This tutorial demonstrates how to create and use a custom environment in nnabla-rl.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "Let's start by first installing nnabla-rl and importing required packages for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nnabla-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnabla as nn\n",
    "from nnabla import functions as NF\n",
    "from nnabla import parametric_functions as NPF\n",
    "import nnabla.solvers as NS\n",
    "\n",
    "import nnabla_rl\n",
    "import nnabla_rl.algorithms as A\n",
    "import nnabla_rl.hooks as H\n",
    "from nnabla_rl.utils.evaluator import EpisodicEvaluator\n",
    "from nnabla_rl.models.q_function import DiscreteQFunction\n",
    "from nnabla_rl.builders import ModelBuilder, SolverBuilder\n",
    "import nnabla_rl.functions as RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding gym.Env\n",
    "\n",
    "If you don't know what gym library is, [gym documentation](https://gym.openai.com/docs/) will be helpful. Please read it before creating an original enviroment.\n",
    "\n",
    "Referring to the [gym.Env implementation](https://github.com/openai/gym/blob/master/gym/core.py), gym Env has following five methods.\n",
    "\n",
    "- `step(action): Run one timestep of the environment's dynamics.` This method's argument is action and this should return next_state, reward, done, and info.\n",
    "\n",
    "- `reset(): Resets the environment to an initial state and returns an initial observation.` \n",
    "\n",
    "- `render(): Renders the environment.` (Optional)\n",
    "\n",
    "- `close(): Override close in your subclass to perform any necessary cleanup.`  (Optional)\n",
    "\n",
    "- `seed(): Sets the seed for this env's random number generator(s).`  (Optional)\n",
    "\n",
    "In addition, there are three key attributes.\n",
    "\n",
    "- `action_space: The Space object corresponding to valid actions.`\n",
    "\n",
    "- `observation_space: The Space object corresponding to valid observations`\n",
    "\n",
    "- `reward_range: A tuple corresponding to the min and max possible rewards`  (Optional)\n",
    "\n",
    "action_space and observation_space should be defined by using [gym.Spaces](https://github.com/openai/gym/tree/master/gym/spaces).\n",
    "\n",
    "These methods and attributes will decide how environment works, so let's implement them!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Enviroment\n",
    "\n",
    "As an example case, we will create a simple enviroment called CliffEnv which has following settings.\n",
    "\n",
    "<img src=\"./assets/CliffEnv.png\" width=\"500\">\n",
    "\n",
    "- In this enviroment, task goal is to reach the place where is 10.0 <= x and 0.0 <= y <= 5.0\n",
    "\n",
    "- State is continuous and has 2 dimension (i.e., x and y).\n",
    "\n",
    "- There are two discrete actions, up (y+=5), right (x+=5).\n",
    "\n",
    "- If agent reaches the cliff region (x > 5.0 and x < 10.0 and y > 0.0 and y < 5.0) or (x < 0.0) or (y > 10.0) or (y < 0.0), -100 is given as reward.\n",
    "\n",
    "- For all timesteps the agent gets -1 as reward.\n",
    "\n",
    "- If agent reaches the goal (x >= 10.0 and y >= 5.0 and y <= 10.0), 100 is given as reward.\n",
    "\n",
    "- Initial states are x=2.5, y=2.5.\n",
    "\n",
    "We can easily guess the optimal actions are \\[ \"up\", \"right\", \"right\" \\] and the optimal score will be 98 (-1 + -1 + 100).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class CliffEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # action is defined as follows:\n",
    "        # 0 = up, 1 = right\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(shape=(2,), low=-np.inf, high=np.inf, dtype=np.float32)\n",
    "        self._state = np.array([2.5, 2.5])\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = np.array([2.5, 2.5])\n",
    "        return self._state\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:  # up (y+=5)\n",
    "            self._state[1] += 5.\n",
    "        elif action == 1:  # right (x+=5)\n",
    "            self._state[0] += 5.\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        x, y = self._state\n",
    "        if (x > 5.0 and y < 5.0) or (x < 0.0) or (y > 10.0) or (y < 0.0):\n",
    "            done = True\n",
    "            reward = -100\n",
    "        elif x >= 10.0 and y >= 5.0 and y <= 10.0:\n",
    "            done = True\n",
    "            reward = 100\n",
    "        else:\n",
    "            done = False\n",
    "            reward = -1\n",
    "\n",
    "        info = {}\n",
    "        return self._state, reward, done, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining an original enviroment, it would be nice to confirm if your implementation is correct by running this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffEnv()\n",
    "\n",
    "# first call reset and every internal state will be initialized\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # random sample from the action space\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    print('next_state=', next_state, 'action=', action, 'reward=', reward, 'done=', done)\n",
    "    if done:\n",
    "        print(\"Episode is Done\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appling nnabla-rl to an original environment\n",
    "\n",
    "Environment is now ready to run the training!!\\\n",
    "Let's apply nnabla-rl algorithms to the created enviroment and train the agent!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Q function, a Q function solver and a solver builder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffQFunction(DiscreteQFunction):\n",
    "    def __init__(self, scope_name: str, n_action: int):\n",
    "        super(CliffQFunction, self).__init__(scope_name)\n",
    "        self._n_action = n_action\n",
    "\n",
    "    def all_q(self, s: nn.Variable) -> nn.Variable:\n",
    "        with nn.parameter_scope(self.scope_name):\n",
    "            h = NF.tanh(NPF.affine(s, 64, name=\"affine-1\"))\n",
    "            h = NF.tanh(NPF.affine(h, 64, name=\"affine-2\"))\n",
    "            q = NPF.affine(h, self._n_action, name=\"pred-q\")\n",
    "        return q\n",
    "\n",
    "class CliffQFunctionBuilder(ModelBuilder[DiscreteQFunction]):\n",
    "    def build_model(self, scope_name, env_info, algorithm_config, **kwargs):\n",
    "        return CliffQFunction(scope_name, env_info.action_dim)\n",
    "\n",
    "class CliffSolverBuilder(SolverBuilder):\n",
    "    def build_solver(self,  # type: ignore[override]\n",
    "                     env_info,\n",
    "                     algorithm_config,\n",
    "                     **kwargs):\n",
    "        return NS.Adam(alpha=algorithm_config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate your env and run the training !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "train_env = CliffEnv()\n",
    "eval_env = CliffEnv()\n",
    "\n",
    "iteration_num_hook = H.IterationNumHook(timing=100)\n",
    "evaluator = EpisodicEvaluator(run_per_evaluation=10)\n",
    "evaluation_hook = H.EvaluationHook(eval_env, evaluator, timing=100)\n",
    "total_timesteps = 10000\n",
    "\n",
    "config = A.DQNConfig(\n",
    "    gpu_id=0,\n",
    "    gamma=0.99,\n",
    "    learning_rate=1e-5,\n",
    "    batch_size=32,\n",
    "    learner_update_frequency=1,\n",
    "    target_update_frequency=1000,\n",
    "    start_timesteps=1000,\n",
    "    replay_buffer_size=1000,\n",
    "    max_explore_steps=10000,\n",
    "    initial_epsilon=1.0,\n",
    "    final_epsilon=0.0,\n",
    "    test_epsilon=0.0,\n",
    ")\n",
    "\n",
    "dqn = A.DQN(train_env, config=config, q_func_builder=CliffQFunctionBuilder(),\n",
    "            q_solver_builder=CliffSolverBuilder())\n",
    "\n",
    "hooks = [iteration_num_hook, evaluation_hook]\n",
    "dqn.set_hooks(hooks)\n",
    "\n",
    "dqn.train_online(train_env, total_iterations=total_timesteps)\n",
    "\n",
    "eval_env.close()\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the agent gets 98 score in evaluation enviroment!! That means we solved the task. Congratuations!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}